{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kJssPtXWHvw",
        "outputId": "3470d06e-3cd1-4193-bc3c-a025f1a9317e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 15119/15119 [48:11<00:00,  5.23it/s, i loss train=0.454, acc=84.52%, step=15118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Epoch 1 | Train Accuracy: 84.52% | Avg Loss: 0.4290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 15119/15119 [48:11<00:00,  5.23it/s, i loss train=0.789, acc=86.72%, step=15118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Epoch 2 | Train Accuracy: 86.72% | Avg Loss: 0.3653\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load and filter dataset\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df['toxic'] = (df['target'] >= 0.5).astype(int)\n",
        "\n",
        "# Define subtype labels\n",
        "subtype_labels = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n",
        "df[subtype_labels] = (df[subtype_labels] >= 0.5).astype(int)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Dataset class\n",
        "class ToxicityDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df['comment_text'].tolist()\n",
        "        self.targets = df['toxic'].tolist()\n",
        "        self.labels = df[subtype_labels].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].squeeze(),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(),\n",
        "            'toxicity': torch.tensor(self.targets[idx], dtype=torch.float),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Model\n",
        "class ToxicityClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.tox_head = nn.Linear(768, 1)              # Binary toxicity score\n",
        "        self.subtype_head = nn.Linear(768, len(subtype_labels))  # Multi-label subtypes\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        toxicity_score = torch.sigmoid(self.tox_head(out)).squeeze(1)\n",
        "        subtype_pred = torch.sigmoid(self.subtype_head(out))\n",
        "        return toxicity_score, subtype_pred\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ToxicityClassifier().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "bce_loss = nn.BCELoss()\n",
        "\n",
        "train_loader = DataLoader(ToxicityDataset(df), batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    for step, batch in enumerate(loop):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        toxicity = batch['toxicity'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        tox_pred, subtype_pred = model(input_ids, attention_mask)\n",
        "\n",
        "        loss_tox = bce_loss(tox_pred, toxicity)\n",
        "        loss_subtype = bce_loss(subtype_pred, labels)\n",
        "        loss = loss_tox + loss_subtype\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = (tox_pred >= 0.5).float()\n",
        "        correct += (preds == toxicity).sum().item()\n",
        "        total += toxicity.size(0)\n",
        "\n",
        "        loop.set_postfix({\n",
        "            \"i loss train\": f\"{loss.item():.3f}\",\n",
        "            \"acc\": f\"{100*correct/total:.2f}%\",\n",
        "            \"step\": step\n",
        "        })\n",
        "\n",
        "    print(f\"✅ Epoch {epoch+1} | Train Accuracy: {100*correct/total:.2f}% | Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    torch.save(model.state_dict(), f\"toxicity.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeX0t48OxXzo",
        "outputId": "90a64713-8d89-4b7d-cd25-a5ff6f2f90c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 56326 validation samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running Evaluation: 100%|██████████| 1761/1761 [02:28<00:00, 11.83it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Toxicity Binary Classification Report ===\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Appropriate       0.88      0.92      0.90     40718\n",
            "Inappropriate       0.77      0.66      0.71     15608\n",
            "\n",
            "     accuracy                           0.85     56326\n",
            "    macro avg       0.82      0.79      0.80     56326\n",
            " weighted avg       0.85      0.85      0.85     56326\n",
            "\n",
            "Toxicity ROC AUC: 0.9000073221174536\n",
            "\n",
            "=== Toxic Subtype Classification Report ===\n",
            "\n",
            "-- SEVERE_TOXICITY --\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56325\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           1.00     56326\n",
            "   macro avg       0.50      0.50      0.50     56326\n",
            "weighted avg       1.00      1.00      1.00     56326\n",
            "\n",
            "\n",
            "-- OBSCENE --\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     55377\n",
            "           1       0.64      0.59      0.62       949\n",
            "\n",
            "    accuracy                           0.99     56326\n",
            "   macro avg       0.82      0.79      0.81     56326\n",
            "weighted avg       0.99      0.99      0.99     56326\n",
            "\n",
            "\n",
            "-- IDENTITY_ATTACK --\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     54767\n",
            "           1       0.52      0.48      0.50      1559\n",
            "\n",
            "    accuracy                           0.97     56326\n",
            "   macro avg       0.75      0.73      0.74     56326\n",
            "weighted avg       0.97      0.97      0.97     56326\n",
            "\n",
            "\n",
            "-- INSULT --\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.93     44908\n",
            "           1       0.78      0.67      0.72     11418\n",
            "\n",
            "    accuracy                           0.89     56326\n",
            "   macro avg       0.85      0.81      0.83     56326\n",
            "weighted avg       0.89      0.89      0.89     56326\n",
            "\n",
            "\n",
            "-- THREAT --\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00     55881\n",
            "           1       0.64      0.34      0.44       445\n",
            "\n",
            "    accuracy                           0.99     56326\n",
            "   macro avg       0.82      0.67      0.72     56326\n",
            "weighted avg       0.99      0.99      0.99     56326\n",
            "\n",
            "\n",
            "-- SEXUAL_EXPLICIT --\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00     55721\n",
            "           1       0.67      0.37      0.48       605\n",
            "\n",
            "    accuracy                           0.99     56326\n",
            "   macro avg       0.83      0.69      0.74     56326\n",
            "weighted avg       0.99      0.99      0.99     56326\n",
            "\n",
            "\n",
            "=== Identity-Sliced Subtype Evaluation (e.g., female, muslim, etc.) ===\n",
            "\n",
            "--- FEMALE (3672 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 0.99\n",
            "identity_attack: Accuracy = 0.94\n",
            "insult: Accuracy = 0.90\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 0.97\n",
            "\n",
            "--- MALE (2784 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 0.99\n",
            "identity_attack: Accuracy = 0.91\n",
            "insult: Accuracy = 0.90\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 0.97\n",
            "\n",
            "--- MUSLIM (1282 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 0.99\n",
            "identity_attack: Accuracy = 0.80\n",
            "insult: Accuracy = 0.91\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 1.00\n",
            "\n",
            "--- BLACK (1359 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 0.99\n",
            "identity_attack: Accuracy = 0.74\n",
            "insult: Accuracy = 0.90\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 1.00\n",
            "\n",
            "--- WHITE (2231 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 0.99\n",
            "identity_attack: Accuracy = 0.77\n",
            "insult: Accuracy = 0.91\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 1.00\n",
            "\n",
            "--- CHRISTIAN (1763 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 1.00\n",
            "identity_attack: Accuracy = 0.90\n",
            "insult: Accuracy = 0.91\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 0.99\n",
            "\n",
            "--- JEWISH (358 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 1.00\n",
            "identity_attack: Accuracy = 0.84\n",
            "insult: Accuracy = 0.94\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 1.00\n",
            "\n",
            "--- HINDU (26 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 1.00\n",
            "identity_attack: Accuracy = 0.81\n",
            "insult: Accuracy = 1.00\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 1.00\n",
            "\n",
            "--- BUDDHIST (37 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 1.00\n",
            "identity_attack: Accuracy = 0.89\n",
            "insult: Accuracy = 0.97\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 1.00\n",
            "\n",
            "--- ATHEIST (117 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 0.99\n",
            "identity_attack: Accuracy = 0.82\n",
            "insult: Accuracy = 0.92\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 0.99\n",
            "\n",
            "--- TRANSGENDER (129 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 0.99\n",
            "identity_attack: Accuracy = 0.80\n",
            "insult: Accuracy = 0.93\n",
            "threat: Accuracy = 0.99\n",
            "sexual_explicit: Accuracy = 0.99\n",
            "\n",
            "--- LATINO (116 samples) ---\n",
            "severe_toxicity: Accuracy = 1.00\n",
            "obscene: Accuracy = 0.99\n",
            "identity_attack: Accuracy = 0.77\n",
            "insult: Accuracy = 0.89\n",
            "threat: Accuracy = 1.00\n",
            "sexual_explicit: Accuracy = 1.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# === Config ===\n",
        "VAL_PATH = \"val.csv\"\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "# === Subtype + Identity Columns ===\n",
        "subtype_labels = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n",
        "identity_labels = ['female', 'male', 'muslim', 'black', 'white', 'christian', 'jewish', 'hindu', 'buddhist', 'atheist', 'transgender', 'latino']\n",
        "\n",
        "# === Load val data ===\n",
        "df_val = pd.read_csv(VAL_PATH)\n",
        "print(f\"Loaded {len(df_val)} validation samples\")\n",
        "\n",
        "# === Tokenizer ===\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# === Dataset ===\n",
        "class ValDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df['comment_text'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].squeeze(),\n",
        "            'attention_mask': enc['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "val_loader = DataLoader(ValDataset(df_val), batch_size=BATCH_SIZE)\n",
        "\n",
        "# === Model ===\n",
        "class ToxicityClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        from transformers import BertModel\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.tox_head = nn.Linear(768, 1)\n",
        "        self.subtype_head = nn.Linear(768, len(subtype_labels))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        tox_score = torch.sigmoid(self.tox_head(out)).squeeze(1)\n",
        "        subtype_preds = torch.sigmoid(self.subtype_head(out))\n",
        "        return tox_score, subtype_preds\n",
        "\n",
        "model = ToxicityClassifier().to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"toxicity.pth\", map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# === Inference ===\n",
        "tox_preds, subtype_preds = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Running Evaluation\"):\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "\n",
        "        tox_out, subtype_out = model(input_ids, attention_mask)\n",
        "        tox_preds.extend(tox_out.cpu().numpy())\n",
        "        subtype_preds.extend(subtype_out.cpu().numpy())\n",
        "\n",
        "df_val[\"toxicity_pred\"] = tox_preds\n",
        "df_val[\"toxicity_label_pred\"] = df_val[\"toxicity_pred\"].apply(lambda x: 1 if x >= THRESHOLD else 0)\n",
        "df_val[\"toxicity_label_true\"] = df_val[\"target\"].apply(lambda x: 1 if x >= THRESHOLD else 0)\n",
        "\n",
        "# === Evaluate main classifier ===\n",
        "print(\"\\n=== Toxicity Binary Classification Report ===\")\n",
        "print(classification_report(df_val[\"toxicity_label_true\"], df_val[\"toxicity_label_pred\"], target_names=[\"Appropriate\", \"Inappropriate\"]))\n",
        "print(\"Toxicity ROC AUC:\", roc_auc_score(df_val[\"toxicity_label_true\"], df_val[\"toxicity_pred\"]))\n",
        "\n",
        "# === Evaluate subtypes ===\n",
        "print(\"\\n=== Toxic Subtype Classification Report ===\")\n",
        "subtype_results = []\n",
        "for i, label in enumerate(subtype_labels):\n",
        "    df_val[f\"{label}_pred\"] = [row[i] for row in subtype_preds]\n",
        "    df_val[f\"{label}_label_pred\"] = df_val[f\"{label}_pred\"].apply(lambda x: 1 if x >= THRESHOLD else 0)\n",
        "    df_val[f\"{label}_label_true\"] = df_val[label].apply(lambda x: 1 if x >= THRESHOLD else 0)\n",
        "\n",
        "    print(f\"\\n-- {label.upper()} --\")\n",
        "    print(classification_report(df_val[f\"{label}_label_true\"], df_val[f\"{label}_label_pred\"]))\n",
        "\n",
        "# === Optional: Identity Sliced Subtype Evaluation ===\n",
        "print(\"\\n=== Identity-Sliced Subtype Evaluation (e.g., female, muslim, etc.) ===\")\n",
        "for identity in identity_labels:\n",
        "    subset = df_val[df_val[identity] >= 0.5]\n",
        "    if len(subset) < 20:\n",
        "        continue\n",
        "    print(f\"\\n--- {identity.upper()} ({len(subset)} samples) ---\")\n",
        "    for label in subtype_labels:\n",
        "        y_true = subset[f\"{label}_label_true\"]\n",
        "        y_pred = subset[f\"{label}_label_pred\"]\n",
        "        acc = (y_true == y_pred).mean()\n",
        "        print(f\"{label}: Accuracy = {acc:.2f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
